{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c703813-d2ee-49c6-94b3-baf25c80e9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "  The drop in new student arrivals has significant financial implications for Canadian institutions that \n",
      "rely heavily on international tuition fees. These changes reflect Canada's commitment to managing its immigration \n",
      "system and maintaining the integrity of its temporary resident programs.!\n",
      "\n",
      "--- 1. Tokenization ---\n",
      "Sentence Tokenization: [' The drop in new student arrivals has significant financial implications for Canadian institutions that \\nrely heavily on international tuition fees.', \"These changes reflect Canada's commitment to managing its immigration \\nsystem and maintaining the integrity of its temporary resident programs.\", '!']\n",
      "Word Tokenization: ['The', 'drop', 'in', 'new', 'student', 'arrivals', 'has', 'significant', 'financial', 'implications', 'for', 'Canadian', 'institutions', 'that', 'rely', 'heavily', 'on', 'international', 'tuition', 'fees', '.', 'These', 'changes', 'reflect', 'Canada', \"'s\", 'commitment', 'to', 'managing', 'its', 'immigration', 'system', 'and', 'maintaining', 'the', 'integrity', 'of', 'its', 'temporary', 'resident', 'programs', '.', '!']\n",
      "\n",
      "--- 2. Stop Words Removal ---\n",
      "Filtered Words: ['drop', 'new', 'student', 'arrivals', 'significant', 'financial', 'implications', 'Canadian', 'institutions', 'rely', 'heavily', 'international', 'tuition', 'fees', 'changes', 'reflect', 'Canada', 'commitment', 'managing', 'immigration', 'system', 'maintaining', 'integrity', 'temporary', 'resident', 'programs']\n",
      "\n",
      "--- 3. Text Normalization ---\n",
      "Stemming: ['drop', 'new', 'student', 'arriv', 'signific', 'financi', 'implic', 'canadian', 'institut', 'reli', 'heavili', 'intern', 'tuition', 'fee', 'chang', 'reflect', 'canada', 'commit', 'manag', 'immigr', 'system', 'maintain', 'integr', 'temporari', 'resid', 'program']\n",
      "Lemmatization: ['drop', 'new', 'student', 'arriv', 'signific', 'financi', 'implic', 'canadian', 'institut', 'reli', 'heavili', 'intern', 'tuition', 'fee', 'chang', 'reflect', 'canada', 'commit', 'manag', 'immigr', 'system', 'maintain', 'integr', 'temporari', 'resid', 'program']\n",
      "\n",
      "--- 4. Parts of Speech (POS) Tagging ---\n",
      "POS Tags: [('drop', 'NN'), ('new', 'JJ'), ('student', 'NN'), ('arrivals', 'NNS'), ('significant', 'JJ'), ('financial', 'JJ'), ('implications', 'NNS'), ('Canadian', 'JJ'), ('institutions', 'NNS'), ('rely', 'VBP'), ('heavily', 'RB'), ('international', 'JJ'), ('tuition', 'NN'), ('fees', 'NNS'), ('changes', 'NNS'), ('reflect', 'VBP'), ('Canada', 'NNP'), ('commitment', 'NN'), ('managing', 'VBG'), ('immigration', 'NN'), ('system', 'NN'), ('maintaining', 'VBG'), ('integrity', 'NN'), ('temporary', 'JJ'), ('resident', 'NN'), ('programs', 'NNS')]\n",
      "\n",
      "--- 5. Named Entity Recognition ---\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mmaxent_ne_chunker\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('maxent_ne_chunker')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mchunkers/maxent_ne_chunker/english_ace_multiclass.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Gokoy/nltk_data'\n    - 'C:\\\\Users\\\\Gokoy\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Gokoy\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Gokoy\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Gokoy\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 67\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# 5. Named Entity Recognition (NER)\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- 5. Named Entity Recognition ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 67\u001b[0m ner_tree \u001b[38;5;241m=\u001b[39m \u001b[43mne_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos_tags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNamed Entities:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(ner_tree)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\__init__.py:183\u001b[0m, in \u001b[0;36mne_chunk\u001b[1;34m(tagged_tokens, binary)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    182\u001b[0m     chunker_pickle \u001b[38;5;241m=\u001b[39m _MULTICLASS_NE_CHUNKER\n\u001b[1;32m--> 183\u001b[0m chunker \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunker_pickle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mparse(tagged_tokens)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mmaxent_ne_chunker\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('maxent_ne_chunker')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mchunkers/maxent_ne_chunker/english_ace_multiclass.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Gokoy/nltk_data'\n    - 'C:\\\\Users\\\\Gokoy\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Gokoy\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Gokoy\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Gokoy\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Lab 3 - Natural Language Processing with NLTK\n",
    "# BAM-3034: Sentiment Analysis & Text Mining\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag, ne_chunk, FreqDist\n",
    "\n",
    "# Download necessary NLTK resources (uncomment if running for the first time)\n",
    "# nltk.download(\"all\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Input Text\n",
    "# ---------------------------------------------\n",
    "text = \"\"\" The drop in new student arrivals has significant financial implications for Canadian institutions that \n",
    "rely heavily on international tuition fees. These changes reflect Canada's commitment to managing its immigration \n",
    "system and maintaining the integrity of its temporary resident programs.\"\"\"\n",
    "\n",
    "print(\"Original Text:\\n\", text)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 1. Tokenization\n",
    "# ----------------------------------------\n",
    "print(\"\\n--- 1. Tokenization ---\")\n",
    "sentences = sent_tokenize(text)\n",
    "terms = word_tokenize(text)\n",
    "\n",
    "print(\"Sentence Tokenization:\", sentences)\n",
    "print(\"Word Tokenization:\", terms)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Stop Words Removal\n",
    "# -------------------------------\n",
    "print(\"\\n--- 2. Stop Words Removal ---\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_words = [x for x in terms if x.lower() not in stop_words and x.isalpha()]\n",
    "\n",
    "print(\"Filtered Words:\", filtered_words)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Text Normalization\n",
    "# -------------------------------\n",
    "print(\"\\n--- 3. Text Normalization ---\")\n",
    "\n",
    "# 3.1 Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(w) for w in filtered_words]\n",
    "print(\"Stemming:\", stemmed_words)\n",
    "\n",
    "# 3.2 Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(w.lower()) for w in stemmed_words]\n",
    "print(\"Lemmatization:\", lemmatized_words)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. POS Tagging\n",
    "# -------------------------------\n",
    "print(\"\\n--- 4. Parts of Speech (POS) Tagging ---\")\n",
    "pos_tags = pos_tag(filtered_words)\n",
    "print(\"POS Tags:\", pos_tags)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5. Named Entity Recognition (NER)\n",
    "# --------------------------------------------------\n",
    "print(\"\\n--- 5. Named Entity Recognition ---\")\n",
    "ner_tree = ne_chunk(pos_tags)\n",
    "print(\"Named Entities:\")\n",
    "print(ner_tree)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 6. Frequency Distribution\n",
    "# ---------------------------------------------\n",
    "print(\"\\n--- 6. Frequency Distribution ---\")\n",
    "freq_dist = FreqDist(filtered_words)\n",
    "print(\"Top 5 Most Common Words:\", freq_dist.most_common(5))\n",
    "freq_dist.plot(10, title=\"Word Frequency Distribution\")\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Synonyms and Antonyms\n",
    "# -------------------------------\n",
    "print(\"\\n--- 7. Synonyms and Antonyms ---\")\n",
    "word_choice = \"pushing\"  # You can change this word\n",
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(word_choice):\n",
    "    for lemma in syn.lemmas():\n",
    "        synonyms.append(lemma.name())\n",
    "        if lemma.antonyms():\n",
    "            antonyms.append(lemma.antonyms()[0].name())\n",
    "\n",
    "print(f\"Word: {word_choice}\")\n",
    "print(\"Synonyms:\", set(synonyms))\n",
    "print(\"Antonyms:\", set(antonyms))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35114923-b019-4b41-bbb5-64d2f9ad8695",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
